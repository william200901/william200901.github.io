<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css">
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/1.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/1.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/1.png">
  <link rel="mask-icon" href="/1.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"right","width":300,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #87cefa; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #ff009e, 0 0 5px #ff009e; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #ff009e;    /*上边框颜色*/
        border-left-color: #ff009e;    /*左边框颜色*/
    }
</style>
  <meta name="description" content="记录Hadoop大数据平台部署的的过程，以及其中学到的一些技术和知识，如hdfs，hive，beeline，spark，sqoop等等。记录自己用到过的一些命令，自己一些知识的理解，以及踩的坑......能够搜到的东西就不多写了，记录些自己遇到的问题，以及解决方法">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop大数据相关技术总结">
<meta property="og:url" content="http://example.com/2022/03/15/hadoop/index.html">
<meta property="og:site_name" content="乡晨的博客">
<meta property="og:description" content="记录Hadoop大数据平台部署的的过程，以及其中学到的一些技术和知识，如hdfs，hive，beeline，spark，sqoop等等。记录自己用到过的一些命令，自己一些知识的理解，以及踩的坑......能够搜到的东西就不多写了，记录些自己遇到的问题，以及解决方法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2022/03/15/hadoop/sqoop-1.png">
<meta property="og:image" content="http://example.com/2022/03/15/hadoop/1.png">
<meta property="og:image" content="http://example.com/2022/03/15/hadoop/2.png">
<meta property="og:image" content="http://example.com/2022/03/15/hadoop/3.png">
<meta property="og:image" content="http://example.com/2022/03/15/hadoop/4.png">
<meta property="og:image" content="http://example.com/2022/03/15/hadoop/5.png">
<meta property="article:published_time" content="2022-03-15T06:35:21.000Z">
<meta property="article:modified_time" content="2022-09-01T09:28:16.655Z">
<meta property="article:author" content="乡晨">
<meta property="article:tag" content="hadoop">
<meta property="article:tag" content="sqoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/03/15/hadoop/sqoop-1.png">

<link rel="canonical" href="http://example.com/2022/03/15/hadoop/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Hadoop大数据相关技术总结 | 乡晨的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <script src="https://cdn.jsdelivr.net/gh/stevenjoezhang/live2d-widget@latest/autoload.js"></script>
  
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">乡晨的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/15/hadoop/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/2.png">
      <meta itemprop="name" content="乡晨">
      <meta itemprop="description" content="向远方，星夜兼程">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="乡晨的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Hadoop大数据相关技术总结
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-15 14:35:21" itemprop="dateCreated datePublished" datetime="2022-03-15T14:35:21+08:00">2022-03-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-09-01 17:28:16" itemprop="dateModified" datetime="2022-09-01T17:28:16+08:00">2022-09-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/technology/" itemprop="url" rel="index"><span itemprop="name">technology</span></a>
                </span>
            </span>

          
            <span id="/2022/03/15/hadoop/" class="post-meta-item leancloud_visitors" data-flag-title="Hadoop大数据相关技术总结" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/03/15/hadoop/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/03/15/hadoop/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>14 分钟</span>
            </span>
            <div class="post-description">记录Hadoop大数据平台部署的的过程，以及其中学到的一些技术和知识，如hdfs，hive，beeline，spark，sqoop等等。记录自己用到过的一些命令，自己一些知识的理解，以及踩的坑......能够搜到的东西就不多写了，记录些自己遇到的问题，以及解决方法</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h2><h3 id="什么是hadoop？"><a href="#什么是hadoop？" class="headerlink" title="什么是hadoop？"></a>什么是hadoop？</h3><p>hadoop是Apache基金会开发的分布式系统基础架构。主要解决大数据的存储和分析计算问题。广义上讲，hadoop通常指hadoop生态圈。</p>
<p>Hadoop2.x 由MapReduce（计算），Yarn（资源调度），HDFS（数据存储），Common（辅助工具）组成</p>
<h3 id="hadoop网页端网址"><a href="#hadoop网页端网址" class="headerlink" title="hadoop网页端网址"></a>hadoop网页端网址</h3><p>网址和hadoop的配置有关,具体服务器,端口号都在配置文件里<br>yarn: http://[ip]:[port]/cluster<br>hadf: http://[ip]:[port]/</p>
<h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><p>HDFS有三个重要角色：NameNode（主节点）、DataNode（数据节点）和Client（客户机）。还有一个Block（块）的概念</p>
<p><strong>NameNode：</strong> 主节点，可看作是hdfs系统的管理者，主要负责管理文件系统的命名空间、集群配置信息和存储块的复制等。NameNode会将文件系统的Meta-data存储在内存中，这些信息主要包括了文件信息、每一个文件对应的文件块的信息和每一个文件块在DataNode的信息等。</p>
<p><strong>DataNode：</strong> 从节点，数据存储基本单元。</p>
<p><strong>Client：</strong> 客户机，个人理解是HDFS系统中，访问问文件的那台服务器</p>
<p><strong>Block：</strong> Block是hdfs基本的读写单元。hdfs中文件被切分为块存储</p>
<h3 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h3><p>资源调度，在hadoop的配置中，有NodeManager和ResourceManager两类。<br><strong>Application Master:</strong> 是每个用户作业的主进程，负责管理作业生命周期，包括动态的增加或者减少资源使用（即Container），管理执行流程，处理故障以及执行其它本地化优化。</p>
<p><strong>NodeManager:</strong> 管理单个节点上的资源、处理来自ResourceManager的命令和处理来自ApplicationMaster的命令。</p>
<p><strong>ResourceManager:</strong> 处理客户端请求，监控NodeManager，启动或监控ApplicationMaster，资源分配与调度</p>
<p>NodeManager管理DataNode、NameNode和SecondaryNameNode，一般有Node的服务器上配置一个NodeManager；ResourceManager可认为是主资源调度程序。</p>
<h3 id="Hadoop的配置"><a href="#Hadoop的配置" class="headerlink" title="Hadoop的配置"></a>Hadoop的配置</h3><p>配置文件位于hadoop安装目录的/etc/hadoop/目录下</p>
<h4 id="Hadoop环境配置-hadoop-env-sh"><a href="#Hadoop环境配置-hadoop-env-sh" class="headerlink" title="Hadoop环境配置 hadoop-env.sh"></a>Hadoop环境配置 hadoop-env.sh</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 配置java环境变量</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144</span><br></pre></td></tr></table></figure>
<h4 id="核心配置文件-core-site-xml"><a href="#核心配置文件-core-site-xml" class="headerlink" title="核心配置文件 core-site.xml"></a>核心配置文件 core-site.xml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">		&lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;value&gt;hdfs:&#x2F;&#x2F;hadoop102:9000&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">		&lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">		&lt;value&gt;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;data&#x2F;tmp&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<h4 id="HDFS配置文件-hdfs-site-xml"><a href="#HDFS配置文件-hdfs-site-xml" class="headerlink" title="HDFS配置文件 hdfs-site.xml"></a>HDFS配置文件 hdfs-site.xml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;!-- 辅助节点访问网址配置 --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;</span><br><span class="line">                &lt;value&gt;cpu-node3:9868&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">        &lt;!-- namenode访问网址配置 --&gt;</span><br><span class="line">         &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.namenode.http-address&lt;&#x2F;name&gt;</span><br><span class="line">                &lt;value&gt;cpu-node1:50070&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">        &lt;!-- namenode 数据存储位置 --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">                &lt;value&gt;file:&#x2F;usr&#x2F;local&#x2F;hadoop-3.3.0&#x2F;hdfs-tmp&#x2F;dfs&#x2F;name&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">        &lt;!-- datanode数据存储位置 --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">                &lt;value&gt;file:&#x2F;usr&#x2F;local&#x2F;hadoop-3.3.0&#x2F;hdfs-tmp&#x2F;dfs&#x2F;data&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">        &lt;!-- 文件最大副本数量 --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">                &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">        &lt;!-- never add a new datanode. Suggest setting never if the number of datanodes for the cluster no more than 3 --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">             &lt;name&gt;dfs.client.block.write.replace-datanode-on-failure.policy&lt;&#x2F;name&gt;</span><br><span class="line">             &lt;value&gt;NEVER&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">             &lt;name&gt;dfs.client.block.write.replace-datanode-on-failure.enable&lt;&#x2F;name&gt;</span><br><span class="line">             &lt;value&gt;false&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">        &lt;!--控制DataNode一次可以打开的文件个数--&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.datanode.max.xcievers&lt;&#x2F;name&gt;</span><br><span class="line">                &lt;value&gt;8192&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="yarn配置文件-yarn-env-sh-yarn-site-xml"><a href="#yarn配置文件-yarn-env-sh-yarn-site-xml" class="headerlink" title="yarn配置文件 yarn-env.sh yarn-site.xml"></a>yarn配置文件 yarn-env.sh yarn-site.xml</h4><p>yarn-env.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 设置java环境变量</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144</span><br></pre></td></tr></table></figure>
<p>yarn-site.xml 新增以下配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Reducer获取数据的方式 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">		&lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;</span><br><span class="line">		&lt;value&gt;hadoop103&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 设置yarn的web地址，可以通过浏览器访问该地址，查看任务日志等，用于调试 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;cpu-node2:8088&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<h4 id="MapReduce配置文件-mapred-env-sh-mapred-site-xml"><a href="#MapReduce配置文件-mapred-env-sh-mapred-site-xml" class="headerlink" title="MapReduce配置文件 mapred-env.sh  mapred-site.xml"></a>MapReduce配置文件 mapred-env.sh  mapred-site.xml</h4><p>mapred-env.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 设置java环境变量</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144</span><br></pre></td></tr></table></figure>
<p>mapred-site.xml 增加以下配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 指定MR运行在Yarn上 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">		&lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">		&lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<h3 id="hadoop启动命令"><a href="#hadoop启动命令" class="headerlink" title="hadoop启动命令"></a>hadoop启动命令</h3><p>第一次启动时，需要格式化NameNode</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure>
<p>群起集群命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 在配置NameNode节点的机器上</span><br><span class="line"># 开启hdfs集群</span><br><span class="line">start-dfs.sh</span><br><span class="line"># 关闭hdfs集群</span><br><span class="line">stop-dfs.sh</span><br><span class="line"></span><br><span class="line"># 在配置ResourceManager的机器上</span><br><span class="line"># 启动yarn集群</span><br><span class="line">start-yarn.sh</span><br><span class="line"># 关闭yarn集群</span><br><span class="line">stop-yarn.sh</span><br></pre></td></tr></table></figure>




<h2 id="beeline"><a href="#beeline" class="headerlink" title="beeline"></a>beeline</h2><h3 id="beeline连接hiveserver2服务器"><a href="#beeline连接hiveserver2服务器" class="headerlink" title="beeline连接hiveserver2服务器"></a>beeline连接hiveserver2服务器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 后台不挂断启动hiveserver2服务器</span><br><span class="line">$ nohup hiveserver2 &gt; hiveserver.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"># 使用beeline连接hive数据库</span><br><span class="line">$ beeline</span><br><span class="line">beeline&gt; !connect jdch:hive2:&#x2F;&#x2F;cpu-node1:10000</span><br><span class="line">beeline&gt; 账号</span><br><span class="line">beeline&gt; 密码</span><br></pre></td></tr></table></figure>


<h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><h3 id="什么是hive"><a href="#什么是hive" class="headerlink" title="什么是hive"></a>什么是hive</h3><h3 id="hive的安装与配置"><a href="#hive的安装与配置" class="headerlink" title="hive的安装与配置"></a>hive的安装与配置</h3><h3 id="hive使用"><a href="#hive使用" class="headerlink" title="hive使用"></a>hive使用</h3><h4 id="sql-hive语句"><a href="#sql-hive语句" class="headerlink" title="sql/hive语句"></a>sql/hive语句</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 清空表，不删除</span><br><span class="line">truncate table [table_name];</span><br><span class="line"></span><br><span class="line"># 分区表删除分区</span><br><span class="line">alter table [table_name] drop partition (year&#x3D;&#39;2021&#39;);</span><br><span class="line"></span><br><span class="line"># 将非分区表数据导入分区表，目前报错，没有节点可用，但节点都正常运行，搜索未解决问题，可能是hdfs空间占满了，但系统输出磁盘占用清空未满。故暂时不用分区表</span><br><span class="line">nohup hive -e &quot;use cmcc;insert into table login_detail partition(province,year,period) select *,province_code,login_year,&#39;1-7&#39; as period from login_detail_1_7;&quot; &gt; data_load.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"># 上一条命令将非分区表数据导入分区表，报错原因很可能是空间不够，因为使用一张较小的表，或者选择特定的省份数据，都成功导入到了分区表中</span><br><span class="line">insert into table login_detail partition(province,year,period) select *, province_code, login_year, &#39;1-7&#39; from login_detail_1_7 where province_code &#x3D; &#39;110000&#39;;</span><br><span class="line"></span><br><span class="line"># 清除分区表period&#x3D;1-7分区的数据</span><br><span class="line">truncate table [table_name] partition(period&#x3D;&#39;1-7&#39;);</span><br><span class="line"></span><br><span class="line"># hive查询排除某一列或某几列的方法</span><br><span class="line"># 语法结构为 select &#96;(列名)?+.+&#96; from (查询结果表) 查询结果表别名</span><br><span class="line"># 其中，查询结果表别名一定要有，否则会报错</span><br><span class="line">set hive.support.quoted.identifiers&#x3D;none;   -- 支持使用hive的正则表达式</span><br><span class="line">select &#96;(user_id|orga_code)?+.+&#96; from (select * from login_detail_figures limit 5) a; # 从查询结果中过滤user_id 和 orga_code 列</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>成功从分区表导入数据到分区表成功的语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into table cmcc.login_detail_type1 partition(province,year,period) select b.*,c.province_code,&#39;$1&#39; as \&#96;year\&#96;,&#39;$2&#39; as \&#96;period\&#96; from (select \&#96;(year|period)?+.+\&#96; from (select * from cmcc.login_detail where year&#x3D;&#39;$1&#39; and period&#x3D;&#39;$2&#39; and appid&#x3D;&#39;$appid&#39; and login_type&#x3D;1) a) b left join cmcc.platform_information c on b.appid&#x3D;c.appid;</span><br></pre></td></tr></table></figure>
<p>其中有几个坑记录一下：</p>
<ol>
<li>linux中用echo命令输入该sql语句到sql文件中时，注意 <strong>`</strong> 号需要转义，否则解析失败，? 和 | 号不需要转义</li>
<li>因为select * 会选取表中所有字段，包含分区字段，故需要先排除原分区表中的分区字段（或者手动敲需要的字段，太麻烦）</li>
<li>‘$1’ 和 ‘$2’ 是命令行参数，是年 和 月份区间，最初没有加 as year 和 as period 时，hive执行该语句，map成功后将结果导入hive分区表时失败，报错为<code>Hive Internal Error: java.util.ConcurrentModificationException(null)</code>, 但总的执行结果确显示成功了，具体原因目前还不清楚。但加上as 取别名后，hive便自动新建了对应分区，并将数据导入到该分区中</li>
<li>第3点的问题，也许是有两个未命名字段，如‘2021’，‘1-7’两个，之前只有一个未命名字段时，成功执行过，新建了对应分区并成功导入了数据</li>
</ol>
<p>动态分区的一个问题：动态分区会产生过多小文件，导致hdfs上block很多，影响hive查询速度<br>可以在hive-site.xml中设置一些参数，进行动态分区调优，小文件合并等等，解决小文件过多的问题</p>
<h4 id="hive参数设置"><a href="#hive参数设置" class="headerlink" title="hive参数设置"></a>hive参数设置</h4><p>hive动态分区功能默认关闭，需开启，在hive终端中使用以下命令打开动态分区功能：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition &#x3D;true;</span><br></pre></td></tr></table></figure>
<p>一些优化配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set hive.limit.optimize.enable&#x3D;true; &#x2F;&#x2F;开启对数据源进行采样的功能</span><br><span class="line">set hive.limit.row.max.size&#x3D;100000; &#x2F;&#x2F;设置最小采样容量。默认10万</span><br><span class="line">set hive.limit.optimize.limit.file&#x3D;10; &#x2F;&#x2F;可抽样的最大文件数。默认10个</span><br></pre></td></tr></table></figure>
<p>注意：set方式设置hive参数属于临时设置，只对当前会话窗口有效</p>
<p>永久性修改hive参数的方法是编辑hive配置文件。进入hive安装目录下conf文件夹，修改<code>hive-site.xml</code>文件，增加以下属性：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- modify limit properties --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.limit.optimize.enable&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.limit.row.max.size&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;100000&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.limit.optimize.limit.file&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;10&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- modify dynamic partition properties --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.exec.dynamic.partition&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.exec.dynamic.partition.mode&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;nonstrict&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p>以上属性优化了hive的limit 查询功能，以及开启动态分区和设置非严格模式动态分区</p>
<p>注意：若通过beeline等工具连接hive数据库，修改属性后，需要重启hiveserver2服务才会更新修改的属性</p>
<h2 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h2><h3 id="什么是Sqoop"><a href="#什么是Sqoop" class="headerlink" title="什么是Sqoop"></a>什么是Sqoop</h3><p>Sqoop是Apache软件基金会提供的开源工具，使用hadoop的MapReduce，进行hadoop和数据库（如Mysql等）之间数据的导入导出。</p>
<h3 id="Sqoop的安装与配置"><a href="#Sqoop的安装与配置" class="headerlink" title="Sqoop的安装与配置"></a>Sqoop的安装与配置</h3><p>Sqoop官网：<a target="_blank" rel="noopener" href="https://sqoop.apache.org/">https://sqoop.apache.org/</a></p>
<ol>
<li>下载并上传至服务器<br> 从官网下载最新稳定版本1.4.7，如图：<br> <img src="/2022/03/15/hadoop/sqoop-1.png"><br> 将压缩包上传至服务器，因为可能需要使用hive等相关的工具，所以最好在安装了hive（以及其他需要用到的一些工具，如ZooKeeper等）的服务器上安装Sqoop。<br> 在服务器上解压压缩包:<br> 解压到当前目录：<br> <code>tar -zxvf filename.tar.gz</code><br> 解压到指定目录：<br> <code>tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /usr/local/</code></li>
<li>修改配置文件<code>sqoop-env.sh</code><br> 进入sqoop目录下conf文件夹，复制配置文件模板：<br> <code>cp sqoop-env-template.sh sqoop-env.sh</code><br> 进入配置文件并设置hadoop和hive的路径： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi sqoop-env.sh</span><br><span class="line"></span><br><span class="line">export HADOOP_COMMON_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop-3.3.0&#x2F;</span><br><span class="line">export HADOOP_MAPRED_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop-3.3.0&#x2F;</span><br><span class="line">export HIVE_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;apache-hive-3.1.2-bin</span><br></pre></td></tr></table></figure></li>
<li>将mysql的驱动包mysql-connector-java-5.1.46.jar 复制到Sqoop安装目录下的lib文件夹中。<br> 下载commons-lang-2.6.jar并放于Sqoop安装目录下lib文件夹中：<br> 清华源下载链接：<br> <a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/apache//commons/lang/binaries/commons-lang-2.6-bin.zip">https://mirrors.tuna.tsinghua.edu.cn/apache//commons/lang/binaries/commons-lang-2.6-bin.zip</a><br> 下载完成后解压，即可得到commons-lang-2.6.jar文件</li>
<li>配置环境变量 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 若&#x2F;etc&#x2F;profile文件不可编辑，需修改为可编辑</span><br><span class="line">chmod -v u+v &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line">vi &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line"># 在最后一行加。其中SQOOP_HOME 为sqoop安装路径</span><br><span class="line">export SQOOP_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;sqoop-1.4.7.bin__hadoop-2.6.0</span><br><span class="line">export PATH&#x3D;$PATH:$SQOOP_HOME&#x2F;bin</span><br><span class="line"></span><br><span class="line"># 让修改立即生效</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>
 说明：/etc/profile 文件配置环境变量永久生效，对所有用户有效；~/.bashrc 文件永久有效，但只对当前用户有效</li>
<li>测试<br> 查看是否配置成功及版本号： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop version</span><br></pre></td></tr></table></figure>
 测试sqoop到mysql的连通性： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases --connect jdbc:mysql:&#x2F;&#x2F;ip:port --username root --password 123456</span><br></pre></td></tr></table></figure>
 其中：ip/port为mysql数据库的ip和端口，root为用户名，123456为密码，根据自己情况输入</li>
<li>集群没有安装Hbase、Hactalog等时Sqoop报warning<br> 修改sqoop安装目录下bin文件夹中configure-sqoop文件，找到以下几行并注释掉： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">## Moved to be a runtime check in sqoop.</span><br><span class="line">#if [ ! -d &quot;$&#123;HBASE_HOME&#125;&quot; ]; then</span><br><span class="line">#  echo &quot;Warning: $HBASE_HOME does not exist! HBase imports will fail.&quot;</span><br><span class="line">#  echo &#39;Please set $HBASE_HOME to the root of your HBase installation.&#39;</span><br><span class="line">#fi</span><br><span class="line"></span><br><span class="line">## Moved to be a runtime check in sqoop.</span><br><span class="line">#if [ ! -d &quot;$&#123;HCAT_HOME&#125;&quot; ]; then</span><br><span class="line">#  echo &quot;Warning: $HCAT_HOME does not exist! HCatalog jobs will fail.&quot;</span><br><span class="line">#  echo &#39;Please set $HCAT_HOME to the root of your HCatalog installation.&#39;</span><br><span class="line">#fi</span><br><span class="line"></span><br><span class="line">#if [ ! -d &quot;$&#123;ACCUMULO_HOME&#125;&quot; ]; then</span><br><span class="line">#  echo &quot;Warning: $ACCUMULO_HOME does not exist! Accumulo imports will fail.&quot;</span><br><span class="line">#  echo &#39;Please set $ACCUMULO_HOME to the root of your Accumulo installation.&#39;</span><br><span class="line">#fi</span><br><span class="line">#if [ ! -d &quot;$&#123;ZOOKEEPER_HOME&#125;&quot; ]; then</span><br><span class="line">#  echo &quot;Warning: $ZOOKEEPER_HOME does not exist! Accumulo imports will fail.&quot;</span><br><span class="line">#  echo &#39;Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.&#39;</span><br><span class="line">#fi</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="Sqoop的使用-amp-踩坑过程"><a href="#Sqoop的使用-amp-踩坑过程" class="headerlink" title="Sqoop的使用&amp;踩坑过程"></a>Sqoop的使用&amp;踩坑过程</h3><h4 id="hive表导出到mysql中"><a href="#hive表导出到mysql中" class="headerlink" title="hive表导出到mysql中"></a>hive表导出到mysql中</h4><p>sqoop 从其他数据库如mysql等导数据到hive为导入，使用<code>import</code>命令，数据从hive到其他数据库为导出，使用<code>export</code>命令，后续使用mysql，指代其他数据库。</p>
<p>目前成功的一条指令为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sqoop export --connect &quot;jdbc:mysql:&#x2F;&#x2F;[mysql服务器ip:端口]&#x2F;[数据库名]?useUnicode&#x3D;true&amp;characterEncoding&#x3D;utf-8&quot; \</span><br><span class="line">--username [用户名] \</span><br><span class="line">--password [密码] \</span><br><span class="line">--table [表名] \</span><br><span class="line">--input-fields-terminated-by &#39;,&#39; \</span><br><span class="line">--null-string &#39;0&#39; --null-non-string &#39;0&#39; \</span><br><span class="line">--export-dir [hdfs上文件存储路径] \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--columns [指定mysql字段的顺序]</span><br></pre></td></tr></table></figure>
<p>[]中的内容根据自己情况替换，其中–columns 参数的中，字段用<code>,</code>分隔，不能带空格，会报错；<br>–num_mappers 参数大概是指定MapReduce使用的map数，默认好像是4个，使用4个据说会报错，2个数据传输不全，但之前我踩其他坑的时候改成了一个，没有出问题，就用1个了，有空可尝试下不同map数的影响；<br>–input-fields-terminated-by 指定hdfs文件中每个字段的分隔符，这里是<code>,</code>号；<br>–null-string ‘0’ –null-non-string ‘0’参数好像是将空字段转换为指定的数据，网上多数答案是用<code>&#39;\\N&#39;</code>，但我当时有其他报错，各种尝试中改成了’0’, 还有两个参数是 –input-null-string 和 –input-null-non-string 目前理解是一个是hive到mysql使用，一个是mysql到hive使用，没详细研究；<br><code>useUnicode=true&amp;characterEncoding=utf-8&quot;</code> 参数为指定编码utf8，不然可能出现中文乱码的问题。此外，建立数据库时，可使用<code>create database [database-name] character set utf8</code>命令，建立默认utf8编码的数据库</p>
<h4 id="sqoop使用sql语句操作远程数据库（建表）"><a href="#sqoop使用sql语句操作远程数据库（建表）" class="headerlink" title="sqoop使用sql语句操作远程数据库（建表）"></a>sqoop使用sql语句操作远程数据库（建表）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">## sqoop 操作mysql建立相应的表</span><br><span class="line">sqoop eval \</span><br><span class="line">--connect &quot;jdbc:mysql:&#x2F;&#x2F;[mysql服务器ip:端口]&#x2F;[数据库名]?useUnicode&#x3D;true&amp;characterEncoding&#x3D;utf-8&quot; \</span><br><span class="line">--username [用户名] \</span><br><span class="line">--password [密码] \</span><br><span class="line">--query &quot;create table if not exists data_process_test(appid varchar(255) NULL,platform_name varchar(255) NULL,user_identity varchar(255) NULL,week_of_year int NULL,avg_times double NULL);&quot;</span><br></pre></td></tr></table></figure>
<p>使用sqoop eval，在参数–query中指定sql语句便可操作mysql数据库</p>
<h4 id="从hive-export-数据到-mysql-的流程"><a href="#从hive-export-数据到-mysql-的流程" class="headerlink" title="从hive export 数据到 mysql 的流程"></a>从hive export 数据到 mysql 的流程</h4><ol>
<li><p>使用hql语句处理hive表中的数据，将处理的结果写入新建的hive表中</p>
</li>
<li><p>在mysql数据库中建立对应的表</p>
</li>
<li><p>用sqoop命令，将处理结果hive表export到mysql表中</p>
</li>
</ol>
<h4 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h4><p>首先不得不吐槽，sqoop在命令行中的错误信息，根本看不出问题所在，linux命令行中报错如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">2022-03-16 20:38:44,625 INFO mapreduce.Job: Job job_1646808313962_0051 running in uber mode : false</span><br><span class="line">2022-03-16 20:38:44,629 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">2022-03-16 20:38:51,916 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">2022-03-16 20:38:52,937 INFO mapreduce.Job: Job job_1646808313962_0051 failed with state FAILED due to: Task failed task_1646808313962_0051_m_000000</span><br><span class="line">Job failed as tasks failed. failedMaps:1 failedReduces:0 killedMaps:0 killedReduces: 0</span><br><span class="line"></span><br><span class="line">2022-03-16 20:38:53,398 INFO mapreduce.Job: Counters: 8</span><br><span class="line">	Job Counters </span><br><span class="line">		Failed map tasks&#x3D;1</span><br><span class="line">		Launched map tasks&#x3D;1</span><br><span class="line">		Rack-local map tasks&#x3D;1</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)&#x3D;4146</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)&#x3D;0</span><br><span class="line">		Total time spent by all map tasks (ms)&#x3D;4146</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks&#x3D;4146</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks&#x3D;4245504</span><br><span class="line">2022-03-16 20:38:53,443 WARN mapreduce.Counters: Group FileSystemCounters is deprecated. Use org.apache.hadoop.mapreduce.FileSystemCounter instead</span><br><span class="line">2022-03-16 20:38:53,453 INFO mapreduce.ExportJobBase: Transferred 0 bytes in 53.0633 seconds (0 bytes&#x2F;sec)</span><br><span class="line">2022-03-16 20:38:53,476 WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.hadoop.mapreduce.TaskCounter instead</span><br><span class="line">2022-03-16 20:38:53,484 INFO mapreduce.ExportJobBase: Exported 0 records.</span><br><span class="line">2022-03-16 20:38:53,484 ERROR mapreduce.ExportJobBase: Export job failed!</span><br><span class="line">2022-03-16 20:38:53,485 ERROR tool.ExportTool: Error during export: </span><br><span class="line">Export job failed!</span><br><span class="line">	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:445)</span><br><span class="line">	at org.apache.sqoop.manager.SqlManager.exportTable(SqlManager.java:931)</span><br><span class="line">	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:80)</span><br><span class="line">	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)</span><br><span class="line">	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)</span><br><span class="line">	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)</span><br><span class="line">	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)</span><br><span class="line">	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)</span><br><span class="line">	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)</span><br><span class="line">	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)</span><br></pre></td></tr></table></figure>
<p>我遇到的，基本都是直接map部分从0到100%，然后出这个错误，但是从这里看不错任何错误的原因，最终在网上找到调试的方法。<br>首先要配置好yarn的web端管理界面。在hadoop安装目录下，进入/etc/hadoop/目录，<code>vi yarn-site.xml</code> 打开yarn的配置文件，增加：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">        &lt;property&gt;</span><br><span class="line">                 &lt;name&gt;yarn.resourcemanager.webapp.address&lt;&#x2F;name&gt;</span><br><span class="line">                 &lt;value&gt;cpu-node2:8088&lt;&#x2F;value&gt;</span><br><span class="line">         &lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p>指定web端yarn管理界面的端口，其中，cpu-node2是配置ResourceManager的节点。<br>用自己电脑中的浏览器访问上述网址，我使用的是win10系统，在<code>C:\Windows\System32\drivers\etc\host</code>文件中配置域名ip映射，将cpu-node2 与 其ip地址建立映射关系，之后便可以在浏览器里使用cpu-node2代替服务器ip。因为在web端的yarn管理界面中，点击任何东西之后跳转的界面，都使用cpu-node2，而非服务器ip，若不配置，则浏览器无法访问新界面。配置好之后，在浏览器里输入web端管理界面网址，便可以进入yarn管理界面了，如图：<br><img src="/2022/03/15/hadoop/1.png" alt="yarn管理界面"></p>
<p>选择失败的一个job，点击对应id，进入以下页面:<br><img src="/2022/03/15/hadoop/2.png"></p>
<p>点击logs按钮，进入日志界面：<br><img src="/2022/03/15/hadoop/3.png"></p>
<p>点击系统日志项：<br><img src="/2022/03/15/hadoop/4.png"></p>
<p>点击here打开全部日志：<br><img src="/2022/03/15/hadoop/5.png"></p>
<p>在这里，我们便可以看到出错信息，原因是一个数据无法解析。出现这个问题的原因有很多，需要自己尝试。可能是编码错误，可能是分隔符用错了，可能是有些数据中有特殊字符无法解析，hive的文件存储格式应该为textfile（默认格式）等等。但我这里虽然也遇到了这个问题，但原因不在此。此前有成功从hive导入到mysql的情况，但是后来发现数据虽然成功导入到mysql了，但是数据的字段顺序是乱的，所以需要使用–columns 参数，人为指定存到mysql中的字段顺序。加了–columns参数后，该问题便成功解决。</p>
<p>遇到的另外一个错误的columns not null 的问题，这个问题主要是mysql中某字段设置了非null，然后由于各种问题，解析出的某字段的数据为null时，就会得到mysql的columns not null 的错误。解决方法为：mysql字段设置可为null，或者通过上述sqoop命令，指定null值替换。</p>

    </div>

    
    
    
      

        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="乡晨 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="乡晨 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/hadoop/" rel="tag"><i class="fa fa-tag"></i> hadoop</a>
              <a href="/tags/sqoop/" rel="tag"><i class="fa fa-tag"></i> sqoop</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/09/Learn-about-Nodejs/" rel="prev" title="What's Nodejs?">
      <i class="fa fa-chevron-left"></i> What's Nodejs?
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/09/12/happiness/" rel="next" title="Happiness">
      Happiness <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <!--
    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 
      src="//music.163.com/outchain/player?type=2&id=1492276661&auto=1&height=66">
    </iframe>
    -->
    <!--
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=110 
        src="//music.163.com/outchain/player?type=0&id=6749618498&auto=1&height=90">
      </iframe>
    -->

      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=110
       src="//music.163.com/outchain/player?type=0&id=7081676511&auto=0&height=90"></iframe>

    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#hadoop"><span class="nav-number">1.</span> <span class="nav-text">hadoop</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFhadoop%EF%BC%9F"><span class="nav-number">1.1.</span> <span class="nav-text">什么是hadoop？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hadoop%E7%BD%91%E9%A1%B5%E7%AB%AF%E7%BD%91%E5%9D%80"><span class="nav-number">1.2.</span> <span class="nav-text">hadoop网页端网址</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HDFS"><span class="nav-number">1.3.</span> <span class="nav-text">HDFS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Yarn"><span class="nav-number">1.4.</span> <span class="nav-text">Yarn</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hadoop%E7%9A%84%E9%85%8D%E7%BD%AE"><span class="nav-number">1.5.</span> <span class="nav-text">Hadoop的配置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-hadoop-env-sh"><span class="nav-number">1.5.1.</span> <span class="nav-text">Hadoop环境配置 hadoop-env.sh</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-core-site-xml"><span class="nav-number">1.5.2.</span> <span class="nav-text">核心配置文件 core-site.xml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HDFS%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-hdfs-site-xml"><span class="nav-number">1.5.3.</span> <span class="nav-text">HDFS配置文件 hdfs-site.xml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#yarn%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-yarn-env-sh-yarn-site-xml"><span class="nav-number">1.5.4.</span> <span class="nav-text">yarn配置文件 yarn-env.sh yarn-site.xml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MapReduce%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-mapred-env-sh-mapred-site-xml"><span class="nav-number">1.5.5.</span> <span class="nav-text">MapReduce配置文件 mapred-env.sh  mapred-site.xml</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hadoop%E5%90%AF%E5%8A%A8%E5%91%BD%E4%BB%A4"><span class="nav-number">1.6.</span> <span class="nav-text">hadoop启动命令</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#beeline"><span class="nav-number">2.</span> <span class="nav-text">beeline</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#beeline%E8%BF%9E%E6%8E%A5hiveserver2%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="nav-number">2.1.</span> <span class="nav-text">beeline连接hiveserver2服务器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hive"><span class="nav-number">3.</span> <span class="nav-text">hive</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFhive"><span class="nav-number">3.1.</span> <span class="nav-text">什么是hive</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hive%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE"><span class="nav-number">3.2.</span> <span class="nav-text">hive的安装与配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hive%E4%BD%BF%E7%94%A8"><span class="nav-number">3.3.</span> <span class="nav-text">hive使用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sql-hive%E8%AF%AD%E5%8F%A5"><span class="nav-number">3.3.1.</span> <span class="nav-text">sql&#x2F;hive语句</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#hive%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="nav-number">3.3.2.</span> <span class="nav-text">hive参数设置</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sqoop"><span class="nav-number">4.</span> <span class="nav-text">Sqoop</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFSqoop"><span class="nav-number">4.1.</span> <span class="nav-text">什么是Sqoop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sqoop%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE"><span class="nav-number">4.2.</span> <span class="nav-text">Sqoop的安装与配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sqoop%E7%9A%84%E4%BD%BF%E7%94%A8-amp-%E8%B8%A9%E5%9D%91%E8%BF%87%E7%A8%8B"><span class="nav-number">4.3.</span> <span class="nav-text">Sqoop的使用&amp;踩坑过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#hive%E8%A1%A8%E5%AF%BC%E5%87%BA%E5%88%B0mysql%E4%B8%AD"><span class="nav-number">4.3.1.</span> <span class="nav-text">hive表导出到mysql中</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sqoop%E4%BD%BF%E7%94%A8sql%E8%AF%AD%E5%8F%A5%E6%93%8D%E4%BD%9C%E8%BF%9C%E7%A8%8B%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%88%E5%BB%BA%E8%A1%A8%EF%BC%89"><span class="nav-number">4.3.2.</span> <span class="nav-text">sqoop使用sql语句操作远程数据库（建表）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8Ehive-export-%E6%95%B0%E6%8D%AE%E5%88%B0-mysql-%E7%9A%84%E6%B5%81%E7%A8%8B"><span class="nav-number">4.3.3.</span> <span class="nav-text">从hive export 数据到 mysql 的流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B8%A9%E5%9D%91"><span class="nav-number">4.3.4.</span> <span class="nav-text">踩坑</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="乡晨"
      src="/images/2.png">
  <p class="site-author-name" itemprop="name">乡晨</p>
  <div class="site-description" itemprop="description">向远方，星夜兼程</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/william200901" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;william200901" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/307383016" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;307383016" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>Bilibili</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">乡晨</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">27k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">25 分钟</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













  

  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'RDq3rBoIal0MmoCSJKyX9IkA-gzGzoHsz',
      appKey     : 'H45I32eUW4wPgVfenSFLtjMa',
      placeholder: "",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>


  <script type="text/javascript" src="/js/src/dytitle.js"></script>
</body>
</html>
